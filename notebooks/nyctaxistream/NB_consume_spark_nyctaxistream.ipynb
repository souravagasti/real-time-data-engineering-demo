{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os\n",
    "from utils.generic import load_environment_variables\n",
    "from utils.generate_data import generate_nyctaxistream_async\n",
    "from utils.kafka import write_to_kafka_async,list_kafka_topics,delete_kafka_topic,create_kafka_topic,close_kafka_admin\n",
    "from utils.spark import start_spark_session,read_from_kafka,read_spark_parquet,write_spark_stream\n",
    "import asyncio\n",
    "from delta.tables import DeltaTable\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load env vars\n",
    "config = load_environment_variables()\n",
    "\n",
    "topicname = \"nyctaxistream\"\n",
    "kafka_bootstrap_servers = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\n",
    "delta_path = os.getenv(\"DELTA_PATH_NYCTAXI\")\n",
    "write_folder_path = os.getenv(\"WRITE_FOLDER_PATH_NYCTAXI\")\n",
    "schema = os.getenv(\"SCHEMA_NYCTAXI\")\n",
    "merge_keys = os.getenv(\"MERGE_KEYS_NYCTAXI\").split(\",\")\n",
    "raw_file_path = os.path.join(os.getenv(\"RAW_PATH_NYCTAXI\"),os.getenv(\"RAW_FILE_NAME\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = start_spark_session()\n",
    "\n",
    "# ===============================\n",
    "#  Spark session tuning configs\n",
    "# ===============================\n",
    "\n",
    "# --- Parallelism and partitioning ---\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")           # Reduce shuffle writers\n",
    "spark.conf.set(\"spark.default.parallelism\", \"8\")              # Match cores (for RDD operations)\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"256m\")    # Smaller data chunks per writer\n",
    "\n",
    "# --- Parquet / Delta write optimizations ---\n",
    "spark.conf.set(\"spark.sql.parquet.enableDictionary\", \"false\") # Disable dictionary encoding (major OOM saver)\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\", \"snappy\")  # Lightweight compression\n",
    "spark.conf.set(\"spark.sql.parquet.writer.maxBlockSize\", \"64m\")   # Smaller write blocks\n",
    "spark.conf.set(\"spark.sql.parquet.writer.maxRowGroupSize\", \"32m\")# Smaller in-memory row groups\n",
    "\n",
    "# # --- Optional Delta-specific safety ---\n",
    "# spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"false\")  # Turn off small-batch aggregation (uses memory)\n",
    "# spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"false\")    # Disable automatic compaction\n",
    "\n",
    "# # --- Networking / timeout resiliency ---\n",
    "# spark.conf.set(\"spark.network.timeout\", \"600s\")               # Prevent premature heartbeats\n",
    "# spark.conf.set(\"spark.executor.heartbeatInterval\", \"60s\")     # Give executors longer to report health\n",
    "# spark.conf.set(\"spark.rpc.message.maxSize\", \"256\")            # Increase for large commits\n",
    "\n",
    "# # --- Arrow / Pandas interop (disable unless you need it) ---\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "\n",
    "# # --- Miscellaneous ---\n",
    "# spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")          # Allow AQE to optimize join/shuffle\n",
    "# spark.conf.set(\"spark.sql.execution.reuseSubquery\", \"true\")   # Avoid recomputation in nested queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Delta table exists\n",
    "# If not, read raw parquet data and write as Delta table\n",
    "try:\n",
    "    if not(DeltaTable.isDeltaTable(spark, delta_path)):\n",
    "        df_raw = read_spark_parquet(spark,raw_file_path,)\n",
    "        df_raw.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "finally:    \n",
    "    print(f\"Delta table exists at {delta_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play with this\n",
    "params_read_from_kafka = {\n",
    "    \"spark\": spark,\n",
    "    \"topic\": topicname,\n",
    "    \"schema\": schema,\n",
    "    \"startingOffsets\": \"earliest\"\n",
    "}\n",
    "\n",
    "df_stream = read_from_kafka(**params_read_from_kafka )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####delete rows where dispatching_base_num is B03494 or B03496 so that we can see them re-ingested from kafka stream\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load the Delta table\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# Delete rows where dispatching_base_num is B03494 or B03496\n",
    "delta_table.delete(\"dispatching_base_num IN ('B03494', 'B03496')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play with this\n",
    "params_write_spark_stream = {\n",
    "\"parsed_df\": df_stream,\n",
    "\"spark\": spark,\n",
    "\"topic\": \"nycstream\",\n",
    "\"write_to\": \"memory\",\n",
    "\"trigger_mode\": \"once\",\n",
    "\"output_path\": delta_path,\n",
    "\"output_mode\": \"append\",\n",
    "\"merge_keys\": merge_keys,\n",
    "\"interval_seconds\": 5,\n",
    "}\n",
    "\n",
    "query = write_spark_stream(**params_write_spark_stream)\n",
    "\n",
    "# Wait until the query finishes\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "    if params_write_spark_stream[\"trigger_mode\"] == \"continuous\": \n",
    "        spark.sql(\"select * from nycstream\").show(10)\n",
    "finally:\n",
    "    if params_write_spark_stream[\"trigger_mode\"] != \"continuous\":\n",
    "        query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from nycstream\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\") \\\n",
    "    .load(delta_path) \\\n",
    "    .filter(\"dispatching_base_num = 'B03494'\") \\\n",
    "    .count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load Delta table\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# Filter rows\n",
    "matching_rows = delta_table.toDF().filter(\"dispatching_base_num IN ('B03494', 'B03496')\")\n",
    "\n",
    "# Show matching rows\n",
    "# matching_rows.show(truncate=False)\n",
    "\n",
    "# If you want the count\n",
    "print(\"Count:\", matching_rows.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####TODO\n",
    "# streaming dataframe has to re-created if kafka starts.. no live viewing.\n",
    "# trigger mode continuous .. foreachbatch not supported\n",
    "# play with startingOffsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-realtime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
