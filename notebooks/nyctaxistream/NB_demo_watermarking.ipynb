{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting kafka topics...\n",
      "Deleting topic nyctaxistream\n",
      "Topic 'nyctaxistream' deleted\n",
      "Starting Spark App: spark_session_adfb65de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/09 22:44:40 WARN Utils: Your hostname, Souravs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)\n",
      "25/12/09 22:44:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/souravagasti/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/souravagasti/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      "org.apache.spark#spark-token-provider-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-fbf1e8a5-0b59-4da9-9a5e-4685f90703b9;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/souravagasti/projects/real-time-data/venv-realtime/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.5.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.5.5-1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      ":: resolution report :: resolve 184ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.5.5-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 by [org.apache.kafka#kafka-clients;3.5.0] in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.0 by [org.xerial.snappy#snappy-java;1.1.10.3] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 by [org.slf4j#slf4j-api;2.0.7] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   0   |   0   |   3   ||   15  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-fbf1e8a5-0b59-4da9-9a5e-4685f90703b9\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 15 already retrieved (0kB/3ms)\n",
      "25/12/09 22:44:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized successfully with Delta support.\n"
     ]
    }
   ],
   "source": [
    "%run ./NB_startup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "window size 15 mins, watermarking 2 mins, max temp, number of drops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting kafka topics...\n",
      "Deleting topic location_temperature\n",
      "Topic 'location_temperature' does not exist\n",
      "Deleting topic nyctaxistream\n",
      "Topic 'nyctaxistream' does not exist\n",
      "Creating Kafka topic location_temperature...\n",
      "Topic 'location_temperature' created\n",
      "Creating Kafka topic nyctaxistream...\n",
      "Topic 'nyctaxistream' created\n",
      "Kafka topics reset complete.\n",
      "Generated: {'locationId': 258, 'temperature': 15, 'record_datetime': '2025-01-01 02:25:16'}\n",
      "Generated: {'locationId': 258, 'temperature': 16, 'record_datetime': '2025-01-01 02:27:00'}\n",
      "Generated: {'locationId': 258, 'temperature': 19, 'record_datetime': '2025-01-01 02:30:10'}\n",
      "Generated: {'locationId': 258, 'temperature': 21, 'record_datetime': '2025-01-01 02:30:10'}\n",
      "Generated: {'locationId': 258, 'temperature': 18, 'record_datetime': '2025-01-01 02:30:10'}\n",
      "Generated: {'hvfhs_license_num': 'HV00007', 'dispatching_base_num': 'B03404', 'originating_base_num': 'B03406', 'request_datetime': '2025-01-01 02:25:16', 'on_scene_datetime': '2025-01-01 02:54:49', 'pickup_datetime': '2025-01-01 03:06:04', 'dropoff_datetime': '2025-01-01 04:09:46', 'PULocationID': 145, 'DOLocationID': 258, 'trip_miles': 1.6, 'trip_time': 3620}\n",
      "Generated: {'hvfhs_license_num': 'HV00005', 'dispatching_base_num': 'B02764', 'originating_base_num': 'B02510', 'request_datetime': '2025-01-01 02:25:16', 'on_scene_datetime': '2025-01-01 02:25:39', 'pickup_datetime': '2025-01-01 02:35:01', 'dropoff_datetime': '2025-01-01 02:38:47', 'PULocationID': 152, 'DOLocationID': 258, 'trip_miles': 12.56, 'trip_time': 6842}\n",
      "Generated: {'hvfhs_license_num': 'HV00005', 'dispatching_base_num': 'B02764', 'originating_base_num': 'B02510', 'request_datetime': '2025-01-01 02:25:16', 'on_scene_datetime': '2025-01-01 02:29:38', 'pickup_datetime': '2025-01-01 02:50:17', 'dropoff_datetime': '2025-01-01 04:25:02', 'PULocationID': 222, 'DOLocationID': 258, 'trip_miles': 7.28, 'trip_time': 1055}\n",
      "Generated: {'hvfhs_license_num': 'HV00000', 'dispatching_base_num': 'B03406', 'originating_base_num': 'B03406', 'request_datetime': '2025-01-01 02:25:16', 'on_scene_datetime': '2025-01-01 02:39:11', 'pickup_datetime': '2025-01-01 02:48:21', 'dropoff_datetime': '2025-01-01 04:06:22', 'PULocationID': 2, 'DOLocationID': 258, 'trip_miles': 4.59, 'trip_time': 5229}\n",
      "Generated: {'hvfhs_license_num': 'HV00001', 'dispatching_base_num': 'B03404', 'originating_base_num': 'B03404', 'request_datetime': '2025-01-01 02:25:16', 'on_scene_datetime': '2025-01-01 02:27:29', 'pickup_datetime': '2025-01-01 02:40:59', 'dropoff_datetime': '2025-01-01 03:13:04', 'PULocationID': 96, 'DOLocationID': 258, 'trip_miles': 0.22, 'trip_time': 3240}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "clean_reset_kafka_topics([\"location_temperature\",\"nyctaxistream\"]) #recreate these two topics\n",
    "\n",
    "location_temperature_task_1 = asyncio.create_task(write_to_kafka_async(\"location_temperature\", generate_locationid_temperature_async(\n",
    "    latency = 1, locationId=[258],record_datetime=[datetime(2025, 1, 1, 2, 25, 16)]\n",
    "    )))\n",
    "\n",
    "location_temperature_task_2 = asyncio.create_task(write_to_kafka_async(\"location_temperature\", generate_locationid_temperature_async(\n",
    "    latency = 1, locationId=[258],record_datetime=[datetime(2025, 1, 1, 2, 27, 0)]\n",
    "    )))\n",
    "\n",
    "location_temperature_task_3 = asyncio.create_task(write_to_kafka_async(\"location_temperature\", generate_locationid_temperature_async(\n",
    "    latency = 1, locationId=[258],record_datetime=[datetime(2025, 1, 1, 2, 30, 10)]\n",
    "    )))\n",
    "\n",
    "location_temperature_task_4 = asyncio.create_task(write_to_kafka_async(\"location_temperature\", generate_locationid_temperature_async(\n",
    "    latency = 1, locationId=[258],record_datetime=[datetime(2025, 1, 1, 2, 30, 10)]\n",
    "    )))\n",
    "\n",
    "location_temperature_task_5 = asyncio.create_task(write_to_kafka_async(\"location_temperature\", generate_locationid_temperature_async(\n",
    "    latency = 1, locationId=[258],record_datetime=[datetime(2025, 1, 1, 2, 30, 10)]\n",
    "    )))\n",
    "\n",
    "nyctaxi_task_1 = asyncio.create_task(write_to_kafka_async(\"nyctaxistream\", generate_nyctaxistream_async(\n",
    "    latency = 1, request_datetime = [datetime(2025, 1, 1, 2, 25, 16)],DOLocationID=[258])))\n",
    "\n",
    "nyctaxi_task_2 = asyncio.create_task(write_to_kafka_async(\"nyctaxistream\", generate_nyctaxistream_async(\n",
    "    latency = 1, request_datetime = [datetime(2025, 1, 1, 2, 25, 16)],DOLocationID=[258])))\n",
    "\n",
    "nyctaxi_task_3 = asyncio.create_task(write_to_kafka_async(\"nyctaxistream\", generate_nyctaxistream_async(\n",
    "    latency = 1, request_datetime = [datetime(2025, 1, 1, 2, 25, 16)],DOLocationID=[258])))\n",
    "\n",
    "nyctaxi_task_4 = asyncio.create_task(write_to_kafka_async(\"nyctaxistream\", generate_nyctaxistream_async(\n",
    "    latency = 1, request_datetime = [datetime(2025, 1, 1, 2, 25, 16)],DOLocationID=[258])))\n",
    "\n",
    "nyctaxi_task_5 = asyncio.create_task(write_to_kafka_async(\"nyctaxistream\", generate_nyctaxistream_async(\n",
    "    latency = 1, request_datetime = [datetime(2025, 1, 1, 2, 25, 16)],DOLocationID=[258])))\n",
    "\n",
    "\n",
    "# await asyncio.gather(location_temperature_task_1,location_temperature_task_2,location_temperature_task_3,\n",
    "#                location_temperature_task_4,location_temperature_task_5,nyctaxi_task_1,nyctaxi_task_2,\n",
    "#                nyctaxi_task_3,nyctaxi_task_4,nyctaxi_task_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|dispatching_base_num|\n",
      "+--------------------+\n",
      "|              B03404|\n",
      "|              B02764|\n",
      "|              B03406|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#startingOffset = Earliest to read from beginning\n",
    "params_read_from_kafka = {\n",
    "    \"spark\": spark,\n",
    "    \"topic\": \"nyctaxistream\",\n",
    "    \"schema\": schema,\n",
    "    \"startingOffsets\": \"earliest\"\n",
    "}\n",
    "\n",
    "df_stream = read_from_kafka(**params_read_from_kafka)\n",
    "\n",
    "df_stream = (\n",
    "    df_stream\n",
    "    .withColumn(\"request_datetime\", to_timestamp(\"request_datetime\"))\n",
    "    .withColumn(\"on_scene_datetime\", to_timestamp(\"on_scene_datetime\"))\n",
    "    .withColumn(\"pickup_datetime\", to_timestamp(\"pickup_datetime\"))\n",
    "    .withColumn(\"dropoff_datetime\", to_timestamp(\"dropoff_datetime\"))\n",
    ")\n",
    "\n",
    "# write to memory\n",
    "params_write_spark_stream = {\n",
    "\"parsed_df\": df_stream,\n",
    "\"spark\": spark,\n",
    "\"topic\": \"nycstream\",       #the in memory df would be of this name\n",
    "\"write_to\": \"memory\",\n",
    "\"trigger_mode\": \"once\",\n",
    "\"output_path\": delta_path,\n",
    "\"output_mode\": \"append\",\n",
    "\"merge_keys\": merge_keys,\n",
    "\"interval_seconds\": 5,\n",
    "}\n",
    "\n",
    "query = write_spark_stream(**params_write_spark_stream)\n",
    "\n",
    "# Wait until the query finishes\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "    # if params_write_spark_stream[\"trigger_mode\"] == \"continuous\": \n",
    "    #     spark.sql(\"select * from nycstream\").show(10)\n",
    "finally:\n",
    "    if params_write_spark_stream[\"trigger_mode\"] != \"continuous\":\n",
    "        query.stop()\n",
    "\n",
    "spark.sql(\"select * from nycstream\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_stream' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# write to memory\u001b[39;00m\n\u001b[32m      2\u001b[39m params_write_spark_stream = {\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mparsed_df\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mdf_stream\u001b[49m,\n\u001b[32m      4\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mspark\u001b[39m\u001b[33m\"\u001b[39m: spark,\n\u001b[32m      5\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mtopic\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mnycstream\u001b[39m\u001b[33m\"\u001b[39m,       \u001b[38;5;66;03m#the in memory df would be of this name\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mwrite_to\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmemory\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mtrigger_mode\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33monce\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m \u001b[33m\"\u001b[39m\u001b[33moutput_path\u001b[39m\u001b[33m\"\u001b[39m: delta_path,\n\u001b[32m      9\u001b[39m \u001b[33m\"\u001b[39m\u001b[33moutput_mode\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mappend\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mmerge_keys\u001b[39m\u001b[33m\"\u001b[39m: merge_keys,\n\u001b[32m     11\u001b[39m \u001b[33m\"\u001b[39m\u001b[33minterval_seconds\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m5\u001b[39m,\n\u001b[32m     12\u001b[39m }\n\u001b[32m     14\u001b[39m query = write_spark_stream(**params_write_spark_stream)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Wait until the query finishes\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df_stream' is not defined"
     ]
    }
   ],
   "source": [
    "# write to memory\n",
    "params_write_spark_stream = {\n",
    "\"parsed_df\": df_stream,\n",
    "\"spark\": spark,\n",
    "\"topic\": \"nycstream\",       #the in memory df would be of this name\n",
    "\"write_to\": \"memory\",\n",
    "\"trigger_mode\": \"once\",\n",
    "\"output_path\": delta_path,\n",
    "\"output_mode\": \"append\",\n",
    "\"merge_keys\": merge_keys,\n",
    "\"interval_seconds\": 5,\n",
    "}\n",
    "\n",
    "query = write_spark_stream(**params_write_spark_stream)\n",
    "\n",
    "# Wait until the query finishes\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "    # if params_write_spark_stream[\"trigger_mode\"] == \"continuous\": \n",
    "    #     spark.sql(\"select * from nycstream\").show(10)\n",
    "finally:\n",
    "    if params_write_spark_stream[\"trigger_mode\"] != \"continuous\":\n",
    "        query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `nyctaxistream` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [nyctaxistream], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mselect * from nyctaxistream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.show(\u001b[32m10\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/real-time-data/venv-realtime/lib/python3.13/site-packages/pyspark/sql/session.py:1631\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args, **kwargs)\u001b[39m\n\u001b[32m   1627\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1628\u001b[39m         litArgs = \u001b[38;5;28mself\u001b[39m._jvm.PythonUtils.toArray(\n\u001b[32m   1629\u001b[39m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[32m   1630\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1632\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1633\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/real-time-data/venv-realtime/lib/python3.13/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/real-time-data/venv-realtime/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `nyctaxistream` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [nyctaxistream], [], false\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from nyctaxistream\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on every 5 mins, with watermarking of 2 mins, record lowest temperature, and count of drops at locatioid = 258\n",
    "# join on locationid with watermarking, count(*) from nyctaxi, min(temp) from temp stream\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-realtime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
